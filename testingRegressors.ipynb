{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Regression Model to Predict Weekly Natural Gas Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook details the exploratory steps to develop a predictive model that investigates and utilizes prior natural gas prices, natural gas storage data, and Global Forecast System Ensemble temperature predictions (for forecast ranges of 1-16 days at the 850 hPa level).\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initial Findings:*\n",
    " - Gradient boosted regression appears to perform the best of the regressors tested (Ridge, Lasso, ElasticNet, SVR, Random Forest, Gradient Boosting).\n",
    " - Linear regressors become strongly focused on the day1prior price, which is understandable, but they fail to utilize much price sensitivity to upcoming temperature anomalies\n",
    " - On the other hand, gradient boosting distributes feature importance much more, and its scoring is improved over all other models tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xlrd is needed to support pandas import of xls files\n",
    "netcdf4 necessary to import GEFS data, which are generously formatted by the NOAA Earth Systems Research Lab \n",
    "at https://www.esrl.noaa.gov/psd/forecasts/reforecast2/download.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing NatGas data\n",
    "---\n",
    "\n",
    "Read historical gas price (daily & weekly frequency) and storage information downloaded from\n",
    "- https://www.eia.gov/dnav/ng/ng_pri_fut_s1_d.htm\n",
    "- https://www.eia.gov/dnav/ng/ng_pri_fut_s1_w.htm\n",
    "- https://www.eia.gov/dnav/ng/ng_stor_wkly_s1_w.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasPrices = pd.read_excel('./data/NG_PRI_FUT_S1_D.xls',sheetname='Data 1')\n",
    "gasPricesWeek = pd.read_excel('./data/NG_PRI_FUT_S1_W.xls',sheetname='Data 1')\n",
    "gasStorage = pd.read_excel('./data/NG_STOR_WKLY_S1_W.xls',sheetname='Data 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean these dataframes by dropping rows with metadata, reseting indices, and re-naming column labels.\n",
    "Also convert price column to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasPrices = gasPrices.drop([0,1]).rename( columns={'Back to Contents':'Date','Data 1: Spot Price':'Price'})\n",
    "gasPrices['Price'] = gasPrices['Price'].astype(float)\n",
    "gasPrices.reset_index(drop=True,inplace=True)\n",
    "\n",
    "gasPricesWeek = gasPricesWeek.drop([0,1]).rename( columns={'Back to Contents':'Date','Data 1: Spot Price':'Price_WeekAvg'})\n",
    "gasPricesWeek['Price_WeekAvg'] = gasPricesWeek['Price_WeekAvg'].astype(float)\n",
    "gasPricesWeek.reset_index(drop=True,inplace=True)\n",
    "\n",
    "gasStorage = gasStorage.drop([0,1]).drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7', \\\n",
    "                            'Unnamed: 8'],axis=1).rename(columns={'Back to Contents':'Date'})\n",
    "gasStorage = gasStorage.rename(columns = {'Data 1: Weekly Working Gas in Underground Storage': 'gasStorage'})\n",
    "gasStorage.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-format Date column so that it can be merged with the temperature data we're about to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gasPrices['Date'] = pd.to_datetime(gasPrices['Date'].dt.strftime('%Y-%m-%d'))\n",
    "gasPricesWeek['Date'] = pd.to_datetime(gasPricesWeek['Date'].dt.strftime('%Y-%m-%d'))\n",
    "gasStorage['Date'] = pd.to_datetime(gasStorage['Date'].dt.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading GEFS forecast temperature data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read downloaded netcdf data from NOAA/ESRL. This is the Day1-8 file (a separate Day9-16 file will be imported shortly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './data/tmp_pres_latlon_mean_20100101_20191231_joeyfVioTi.nc'\n",
    "data = nc.Dataset(file,mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the variables within this nc file. We're mainly interested in Temperature and intTime as these will be used in our dataframe. Pressure is nothing but '850' because we requested the data only at the 850 level. We also don't really care about lat/lon because we're simply taking a mean of the entire domain for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable 1: time\n",
      "Variable 2: intTime\n",
      "Variable 3: lat\n",
      "Variable 4: lon\n",
      "Variable 5: fhour\n",
      "Variable 6: intValidTime\n",
      "Variable 7: pressure\n",
      "Variable 8: Temperature\n"
     ]
    }
   ],
   "source": [
    "for idx, var in enumerate(data.variables):\n",
    "    print(f'Variable {idx+1}: {var}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty arrays to hold the model run times and also the forecast temperature data for each day.\n",
    "Loop through each of the forecast times with the outer loop. Append that forecast time to the modelRunDay list, which we will use as our Date column to merge with NatGas data. The inner loop iterates through the forecast valid times that we want -- [4,12,20,26,...,42] are the positions of 12Z valid times for D1 through D8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRunDay, week1 = [], [[],[],[],[],[],[],[],[]]\n",
    "\n",
    "for i in range(len(data.variables['intTime'][:])):\n",
    "    \n",
    "    modelRunDay.append(data.variables['intTime'][i])\n",
    "    \n",
    "    for idx, time in enumerate([4,12,20,26,30,34,38,42]):\n",
    "        \n",
    "        week1[idx].append(np.mean(data.variables['Temperature'][i][time]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the arrays into two dataframes and join them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempH85 = pd.DataFrame(week1).transpose().rename(columns={0:'D1',1:'D2',2:'D3',3:'D4',4:'D5',5:'D6',6:'D7',7:'D8'})\n",
    "runDate = pd.DataFrame(modelRunDay, columns=['RunDate'])\n",
    "\n",
    "week1data = runDate.join(tempH85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at the dataframe for \"Week 1\" -- note that temperature is in Kelvin, but no need to convert. Kelvin works for now and we'll scale the data in pre-processing prior to feeding it to our various regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RunDate</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010010100</td>\n",
       "      <td>271.684</td>\n",
       "      <td>268.322</td>\n",
       "      <td>268.512</td>\n",
       "      <td>269.092</td>\n",
       "      <td>268.85</td>\n",
       "      <td>268.8</td>\n",
       "      <td>268.317</td>\n",
       "      <td>268.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010010200</td>\n",
       "      <td>268.152</td>\n",
       "      <td>268.244</td>\n",
       "      <td>269.368</td>\n",
       "      <td>269.375</td>\n",
       "      <td>269.219</td>\n",
       "      <td>268.166</td>\n",
       "      <td>267.612</td>\n",
       "      <td>268.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010010300</td>\n",
       "      <td>267.909</td>\n",
       "      <td>269.041</td>\n",
       "      <td>269.242</td>\n",
       "      <td>269.461</td>\n",
       "      <td>268.22</td>\n",
       "      <td>267.155</td>\n",
       "      <td>266.474</td>\n",
       "      <td>267.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010010400</td>\n",
       "      <td>268.845</td>\n",
       "      <td>269.118</td>\n",
       "      <td>269.2</td>\n",
       "      <td>268.011</td>\n",
       "      <td>266.414</td>\n",
       "      <td>266.214</td>\n",
       "      <td>267.794</td>\n",
       "      <td>269.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010010500</td>\n",
       "      <td>268.844</td>\n",
       "      <td>269.123</td>\n",
       "      <td>268.548</td>\n",
       "      <td>267.311</td>\n",
       "      <td>267.009</td>\n",
       "      <td>268.89</td>\n",
       "      <td>269.888</td>\n",
       "      <td>270.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010010600</td>\n",
       "      <td>269.236</td>\n",
       "      <td>268.874</td>\n",
       "      <td>267.697</td>\n",
       "      <td>266.899</td>\n",
       "      <td>268.713</td>\n",
       "      <td>269.772</td>\n",
       "      <td>269.966</td>\n",
       "      <td>271.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010010700</td>\n",
       "      <td>269.143</td>\n",
       "      <td>268.041</td>\n",
       "      <td>267.285</td>\n",
       "      <td>268.91</td>\n",
       "      <td>269.864</td>\n",
       "      <td>269.711</td>\n",
       "      <td>271.664</td>\n",
       "      <td>274.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010010800</td>\n",
       "      <td>268.048</td>\n",
       "      <td>267.391</td>\n",
       "      <td>269.361</td>\n",
       "      <td>270.418</td>\n",
       "      <td>270.94</td>\n",
       "      <td>273.49</td>\n",
       "      <td>275.526</td>\n",
       "      <td>275.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010010900</td>\n",
       "      <td>267.286</td>\n",
       "      <td>269.224</td>\n",
       "      <td>270.189</td>\n",
       "      <td>271.224</td>\n",
       "      <td>274.246</td>\n",
       "      <td>275.433</td>\n",
       "      <td>275.199</td>\n",
       "      <td>274.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010011000</td>\n",
       "      <td>269.171</td>\n",
       "      <td>269.707</td>\n",
       "      <td>270.809</td>\n",
       "      <td>275.053</td>\n",
       "      <td>276.126</td>\n",
       "      <td>275.37</td>\n",
       "      <td>274.347</td>\n",
       "      <td>274.406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RunDate       D1       D2       D3       D4       D5       D6       D7  \\\n",
       "0  2010010100  271.684  268.322  268.512  269.092   268.85    268.8  268.317   \n",
       "1  2010010200  268.152  268.244  269.368  269.375  269.219  268.166  267.612   \n",
       "2  2010010300  267.909  269.041  269.242  269.461   268.22  267.155  266.474   \n",
       "3  2010010400  268.845  269.118    269.2  268.011  266.414  266.214  267.794   \n",
       "4  2010010500  268.844  269.123  268.548  267.311  267.009   268.89  269.888   \n",
       "5  2010010600  269.236  268.874  267.697  266.899  268.713  269.772  269.966   \n",
       "6  2010010700  269.143  268.041  267.285   268.91  269.864  269.711  271.664   \n",
       "7  2010010800  268.048  267.391  269.361  270.418   270.94   273.49  275.526   \n",
       "8  2010010900  267.286  269.224  270.189  271.224  274.246  275.433  275.199   \n",
       "9  2010011000  269.171  269.707  270.809  275.053  276.126   275.37  274.347   \n",
       "\n",
       "        D8  \n",
       "0  268.118  \n",
       "1  268.127  \n",
       "2  267.185  \n",
       "3  269.339  \n",
       "4   270.21  \n",
       "5  271.612  \n",
       "6  274.069  \n",
       "7  275.384  \n",
       "8  274.869  \n",
       "9  274.406  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week1data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data for \"week 2\" i.e. D9-16 temp data\n",
    "Note that we've changed the positional indices in the inner loop to match 12Z for D9-16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './data/tmp_pres_latlon_mean_20100101_20191231_joeyfVioTi_t190.nc'\n",
    "data = nc.Dataset(file,mode='r')\n",
    "\n",
    "modelRunDay, week2 = [], [[],[],[],[],[],[],[],[]]\n",
    "\n",
    "for i in range(len(data.variables['intTime'][:])):\n",
    "    \n",
    "    modelRunDay.append(data.variables['intTime'][i])\n",
    "    \n",
    "    for idx, time in enumerate([3,7,11,15,19,23,27,31]):\n",
    "        \n",
    "        week2[idx].append(np.mean(data.variables['Temperature'][i][time]))\n",
    "        \n",
    "tempH85 = pd.DataFrame(week2).transpose().rename(columns={0:'D9',1:'D10',2:'D11',3:'D12',\n",
    "                                                          4:'D13',5:'D14',6:'D15',7:'D16'})\n",
    "runDate = pd.DataFrame(modelRunDay, columns=['RunDate'])\n",
    "\n",
    "week2data = runDate.join(tempH85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure everything looks OK, here's some week2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RunDate</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>D16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010010100</td>\n",
       "      <td>268.419</td>\n",
       "      <td>268.76</td>\n",
       "      <td>269.685</td>\n",
       "      <td>269.67</td>\n",
       "      <td>270.269</td>\n",
       "      <td>270.079</td>\n",
       "      <td>270.129</td>\n",
       "      <td>271.228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010010200</td>\n",
       "      <td>270.101</td>\n",
       "      <td>271.26</td>\n",
       "      <td>271.354</td>\n",
       "      <td>271.467</td>\n",
       "      <td>272.021</td>\n",
       "      <td>271.985</td>\n",
       "      <td>271.618</td>\n",
       "      <td>271.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010010300</td>\n",
       "      <td>268.929</td>\n",
       "      <td>270.522</td>\n",
       "      <td>271.462</td>\n",
       "      <td>272.581</td>\n",
       "      <td>273.227</td>\n",
       "      <td>273.034</td>\n",
       "      <td>272.112</td>\n",
       "      <td>271.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010010400</td>\n",
       "      <td>270.699</td>\n",
       "      <td>271.57</td>\n",
       "      <td>272.032</td>\n",
       "      <td>272.029</td>\n",
       "      <td>272.127</td>\n",
       "      <td>272.516</td>\n",
       "      <td>272.106</td>\n",
       "      <td>272.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010010500</td>\n",
       "      <td>271.412</td>\n",
       "      <td>272.904</td>\n",
       "      <td>274.181</td>\n",
       "      <td>274.567</td>\n",
       "      <td>274.529</td>\n",
       "      <td>273.659</td>\n",
       "      <td>273.305</td>\n",
       "      <td>274.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010010600</td>\n",
       "      <td>273.521</td>\n",
       "      <td>274.318</td>\n",
       "      <td>274.573</td>\n",
       "      <td>274.444</td>\n",
       "      <td>273.172</td>\n",
       "      <td>271.933</td>\n",
       "      <td>271.817</td>\n",
       "      <td>271.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010010700</td>\n",
       "      <td>274.878</td>\n",
       "      <td>275.013</td>\n",
       "      <td>274.389</td>\n",
       "      <td>273.035</td>\n",
       "      <td>272.088</td>\n",
       "      <td>272.088</td>\n",
       "      <td>272.553</td>\n",
       "      <td>273.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010010800</td>\n",
       "      <td>274.883</td>\n",
       "      <td>273.94</td>\n",
       "      <td>272.321</td>\n",
       "      <td>271.791</td>\n",
       "      <td>272.25</td>\n",
       "      <td>273.083</td>\n",
       "      <td>273.897</td>\n",
       "      <td>274.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010010900</td>\n",
       "      <td>274.847</td>\n",
       "      <td>273.509</td>\n",
       "      <td>271.989</td>\n",
       "      <td>272.195</td>\n",
       "      <td>273.647</td>\n",
       "      <td>275.087</td>\n",
       "      <td>275.748</td>\n",
       "      <td>275.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010011000</td>\n",
       "      <td>274.416</td>\n",
       "      <td>275.427</td>\n",
       "      <td>276.986</td>\n",
       "      <td>278.519</td>\n",
       "      <td>279.588</td>\n",
       "      <td>279.671</td>\n",
       "      <td>278.725</td>\n",
       "      <td>276.574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RunDate       D9      D10      D11      D12      D13      D14      D15  \\\n",
       "0  2010010100  268.419   268.76  269.685   269.67  270.269  270.079  270.129   \n",
       "1  2010010200  270.101   271.26  271.354  271.467  272.021  271.985  271.618   \n",
       "2  2010010300  268.929  270.522  271.462  272.581  273.227  273.034  272.112   \n",
       "3  2010010400  270.699   271.57  272.032  272.029  272.127  272.516  272.106   \n",
       "4  2010010500  271.412  272.904  274.181  274.567  274.529  273.659  273.305   \n",
       "5  2010010600  273.521  274.318  274.573  274.444  273.172  271.933  271.817   \n",
       "6  2010010700  274.878  275.013  274.389  273.035  272.088  272.088  272.553   \n",
       "7  2010010800  274.883   273.94  272.321  271.791   272.25  273.083  273.897   \n",
       "8  2010010900  274.847  273.509  271.989  272.195  273.647  275.087  275.748   \n",
       "9  2010011000  274.416  275.427  276.986  278.519  279.588  279.671  278.725   \n",
       "\n",
       "       D16  \n",
       "0  271.228  \n",
       "1   271.63  \n",
       "2  271.353  \n",
       "3  272.298  \n",
       "4   274.13  \n",
       "5  271.661  \n",
       "6  273.142  \n",
       "7  274.448  \n",
       "8  275.646  \n",
       "9  276.574  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week2data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging our weekly dataframs together to form our \"master\" temp dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterTempData = week1data.merge(week2data, on='RunDate', how='left')\n",
    "masterTempData['Date'] = pd.to_datetime(masterTempData['RunDate'].astype(str),format='%Y%m%d%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge temp and natgas data & prep rows for modeling (e.g., set up lags)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop 'RunDate' since it's no longer needed and also merge our weekly prices (which will eventually form our target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = masterTempData.merge(gasPrices, on='Date',how='left').drop(['RunDate'],axis=1)\n",
    "everything = everything.merge(gasPricesWeek,on='Date',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start creating our \"lag\" columns. The last several days (and perhaps more) of 850 temps may have some impact on upcoming natgas prices so let's appropriately shift the D1 column to make prior day features. \n",
    "\n",
    "---\n",
    "*Note: we are not using observations for prior days. This might be more appropriate, but D1 12-hour ensemble means should be fairly accurate on the scale we need. Given we've already found/loaded the data, let's just use that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything['dayPrior1'] = everything['D1'].shift(1)\n",
    "everything['dayPrior2'] = everything['D1'].shift(2)\n",
    "everything['dayPrior3'] = everything['D1'].shift(3)\n",
    "everything['dayPrior4'] = everything['D1'].shift(4)\n",
    "everything['dayPrior5'] = everything['D1'].shift(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also shift our natgas prices to make new lagged columns. Obviously, next week's average price should be pretty sensitive to recent prices so we want to include those. The number of lags we use is arbitrary of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything['priceLag1'] = everything['Price'].shift(1)\n",
    "everything['priceLag2'] = everything['Price'].shift(2)\n",
    "everything['priceLag3'] = everything['Price'].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift our weekly prices up by 7 (1 week) and 14 rows (2 weeks) to form our targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything['weekAvgPriceLag-1'] = everything['Price_WeekAvg'].shift(-7)\n",
    "everything['weekAvgPriceLag-2'] = everything['Price_WeekAvg'].shift(-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some trend columns for the hell of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are assuming we won't have the current day's closing price -- thus we use the first lag and second lag for the\n",
    "# one day trend and first lag minus third lag for the two-day trend\n",
    "everything['oneDayTrend'] = everything['priceLag1'] - everything['priceLag2']\n",
    "everything['twoDayTrend'] = everything['priceLag1'] - everything['priceLag3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge gas storage data with our main dataframe as well. Create a couple more lag columns too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "everythingPlusStorage = everything.merge(gasStorage,on='Date',how='left')\n",
    "\n",
    "everythingPlusStorage['storageLag1'] = everythingPlusStorage['gasStorage'].shift(7)\n",
    "everythingPlusStorage['storageLag2'] = everythingPlusStorage['gasStorage'].shift(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cleaning step is necessary to remove rows that have \"--\" due to missing model forecast data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "everythingPlusStorageCleaned = everythingPlusStorage[(everythingPlusStorage['D9'] > 0) \\\n",
    "                                                    & (everythingPlusStorage['D1'] > 0)\n",
    "                                                    & (everythingPlusStorage['dayPrior1'] > 0) \\\n",
    "                                                    & (everythingPlusStorage['dayPrior2'] > 0) \\\n",
    "                                                    & (everythingPlusStorage['dayPrior3'] > 0) \\\n",
    "                                                    & (everythingPlusStorage['dayPrior4'] > 0) \\\n",
    "                                                    & (everythingPlusStorage['dayPrior5'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two dataframes that we'll use for our models. The plan is to make a model for next week's prediction, and then a model for the two week prediction. That could change in the future but we'll leave it at that for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1prediction = everythingPlusStorageCleaned.drop(['weekAvgPriceLag-2','Price_WeekAvg',\\\n",
    "                                  'gasStorage'],axis=1)\n",
    "week2prediction = everythingPlusStorageCleaned.drop(['weekAvgPriceLag-1','Price_WeekAvg',\\\n",
    "                                  'gasStorage'],axis=1)\n",
    "\n",
    "week1prediction = week1prediction.dropna(how='any')\n",
    "week2prediction = week2prediction.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Modeling\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we'll use a combo of some regression models with gridsearchcv and some model classes with a built-in cv. Why? Partially because I was just dinking around it with initially, but it's handy to get a feel for both ways. According to sklearn's documentation, classes with built-in cv may perform a little better than their straight-up counterparts at times: <br>\n",
    "*The advantage of using a cross-validation estimator over the canonical Estimator class along with grid search is that they can take advantage of warm-starting by reusing precomputed results in the previous steps of the cross-validation process*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LassoCV, ElasticNetCV \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a quick function to print mean squared error and the coefficient of determination when it comes time to check our models against the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scorer(target, prediction):\n",
    "    \n",
    "    print(f'The mean squared error is: {mean_squared_error(target,prediction)}')\n",
    "    print(f'The coefficient of determination (r2) is: {r2_score(target,prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our regressors and our target variable to begin development for week 1 prediction. For week 1, we will drop from X...\n",
    "- Date: At this time, we'll leave it out. Perhaps we could use the week or month down the line.\n",
    "- Price: We won't have the closing price for the current day when we run our model. So we gotta kick it to the curb.\n",
    "- weekAvgPriceLag-1: This is our target variable so we have to drop this from X.\n",
    "\n",
    "For Y, we set it to our target: weekAvgPriceLag-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = week1prediction.drop(['Date','Price','weekAvgPriceLag-1'],axis=1)\n",
    "Y = week1prediction['weekAvgPriceLag-1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our features and targets into training and test data, with an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our scaler object, then use fit_transform to fit the scaler and transform X_train & X_test. We transform the data by representing them as standardized anomalies (i.e., subtract the mean and divide by the standard deviation). Many ML models perform better if we scale the features in such a manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a quick function to print the attributes of the best estimator when we perform cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCVbest(modelObject):\n",
    "    print(f'Best estimator: {modelObject.best_estimator_}')\n",
    "    print(f'Best parameters: {modelObject.best_params_}')\n",
    "    print(f'Best score: {modelObject.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressors\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our ridge regression oject and a list of alpha parameters. Ridge uses least squares regression, but with a normalization of alpha*||w||^2_2 -- alpha regulates the amount of regularization that occurs.\n",
    "\n",
    "Perform a grid search against the supplied alpha parameters to find the best scoring alpha. Also perform this on k-fold of 5. Lastly, we fit the regressor object to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Best parameters: {'alpha': 10}\n",
      "Best score: -0.03891558775657337\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(solver='auto')\n",
    "parameters = {'alpha':[1e-2,1,5,10,20,40]}\n",
    "ridgeRegressor = GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "ridgeRegressor.fit(train_scaled,y_train)\n",
    "\n",
    "printCVbest(ridgeRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best estimator to predict next week's average natgas price and use our model_scorer function to print the mean squared error and coefficient of determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is: 0.1534526489299111\n",
      "The coefficient of determination (r2) is: 0.8072589128074912\n"
     ]
    }
   ],
   "source": [
    "bestRidge = ridgeRegressor.best_estimator_\n",
    "predictionRidge = bestRidge.predict(test_scaled)\n",
    "\n",
    "model_scorer(y_test,predictionRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "Lasso regression uses a different regularization: alpha*||w||. With this change, minimization of the loss function is useful for feature selection and avoiding overfitting, as it drives many coefficients to 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=[0.01, 1, 5, 10, 20, 40], copy_X=True, cv=5, eps=0.001,\n",
       "    fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1,\n",
       "    normalize=False, positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoRegressor = LassoCV(alphas=[1e-2,1,5,10,20,40],cv=5)\n",
    "lassoRegressor.fit(train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is: 0.15144733172193925\n",
      "The coefficient of determination (r2) is: 0.8097776508125085\n"
     ]
    }
   ],
   "source": [
    "predictionLasso = lassoRegressor.predict(test_scaled)\n",
    "model_scorer(y_test,predictionLasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Regression\n",
    "ElasticNet regression combines the regularizers from Ridge and Lasso regression. The parameter uses a ratio parameter to achieve this hybrid function we wish to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=[0.01, 1, 5, 10, 20, 40], copy_X=True, cv=5, eps=0.001,\n",
       "       fit_intercept=True, l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
       "       max_iter=1000, n_alphas=100, n_jobs=1, normalize=False,\n",
       "       positive=False, precompute='auto', random_state=None,\n",
       "       selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticRegressor = ElasticNetCV(alphas=[1e-2,1,5,10,20,40],l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5)\n",
    "elasticRegressor.fit(train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is: 0.15144733172193925\n",
      "The coefficient of determination (r2) is: 0.8097776508125085\n"
     ]
    }
   ],
   "source": [
    "predictionElastic = elasticRegressor.predict(test_scaled)\n",
    "model_scorer(y_test,predictionElastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the scores for Lasso and ElasticNet are the same. If we print out the coefficients from the models chosen by CV for each of these regressions, we notice they're indeed the same. ElasticNetCV settled on an l1_ratio of 1, which means it's essentially Lasso at this point. So for linear regression, large importance is placed on the prior day's natgas price, and not much else. As you'll see, we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.79218602,  0.        ,  0.0180529 ,  0.        ,\n",
       "        -0.        , -0.        , -0.        ]),\n",
       " array([ 0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        ,  0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.79218602,  0.        ,  0.0180529 ,  0.        ,\n",
       "        -0.        , -0.        , -0.        ]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticRegressor.coef_, lassoRegressor.coef_, elasticRegressor.l1_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model fits a number of decision trees on various sub-sets of the data, thereby creating an ensemble of trees. The decision is then averaged from the trees.\n",
    "\n",
    "---\n",
    "Here we'll use cross validation to test several n_estimators in our regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [5,10,25,50,75,100,150,200,300,400]\n",
    "param_grid = dict(n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 25, 50, 75, 100, 150, 200, 300, 400]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfRegressor = RandomForestRegressor()\n",
    "gridSearched = GridSearchCV(rfRegressor,param_grid,scoring='neg_mean_squared_error',cv=5)\n",
    "gridSearched.fit(train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the importance of features in this ensemble. For this random forest, it acts very similar to the linear regressors, in that it places almost all importance on the prior days' prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1: 0.000863858801516324\n",
      "D2: 0.001001498337021604\n",
      "D3: 0.0017828628032362458\n",
      "D4: 0.001061301466399699\n",
      "D5: 0.0016337496784022943\n",
      "D6: 0.0019637870428739247\n",
      "D7: 0.0012144638439771636\n",
      "D8: 0.0009524505041163913\n",
      "D9: 0.0008186282748865373\n",
      "D10: 0.001061967021934457\n",
      "D11: 0.0014605286512548137\n",
      "D12: 0.0009148201009567135\n",
      "D13: 0.0013758124885811436\n",
      "D14: 0.0007954145747766344\n",
      "D15: 0.0006800143227425303\n",
      "D16: 0.0010446697300350773\n",
      "dayPrior1: 0.0011016768552147723\n",
      "dayPrior2: 0.0007145163299351637\n",
      "dayPrior3: 0.0008605261796679624\n",
      "dayPrior4: 0.0011093132447349773\n",
      "dayPrior5: 0.0012227562977073397\n",
      "priceLag1: 0.5016168416362438\n",
      "priceLag2: 0.3488349121983397\n",
      "priceLag3: 0.1150893942460285\n",
      "oneDayTrend: 0.00209740440315335\n",
      "twoDayTrend: 0.0023013904634145836\n",
      "storageLag1: 0.0039365086125947414\n",
      "storageLag2: 0.002488931890253549\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of each feature in the random forest model\n",
    "for i in range(len(X.columns)):\n",
    "    print(f'{X.columns[i]}: {gridSearched.best_estimator_.feature_importances_[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is: 0.15905823187857968\n",
      "The coefficient of determination (r2) is: 0.8002181340434331\n"
     ]
    }
   ],
   "source": [
    "predictionRF = gridSearched.predict(test_scaled)\n",
    "model_scorer(y_test,predictionRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting builds trees as it iterates forward and fits a tree with each step (controlled by the learning rate) along the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "n_estimators = [5,10,25,50,75,100,150,200,300,400]\n",
    "param_grid = dict(learning_rate=learning_rate,n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3], 'n_estimators': [5, 10, 25, 50, 75, 100, 150, 200, 300, 400]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbRegressor = GradientBoostingRegressor()\n",
    "gridSearched = GridSearchCV(gbRegressor,param_grid,scoring='neg_mean_squared_error',cv=5)\n",
    "gridSearched.fit(train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we print out the importance placed on each feature. Except this time, we notice temperature features are at least an order of magnitude higher in most cases. While most importance is still placed on the prior days' prices, this at least offers a notable change from the prior models. And our scoring suggests gradient boosting may perform the best out of the models tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1: 0.011588540643157659\n",
      "D2: 0.0034521042772727908\n",
      "D3: 0.0034426331127390313\n",
      "D4: 0.00675513456833057\n",
      "D5: 0.04544268373917919\n",
      "D6: 0.02629212934265126\n",
      "D7: 0.006220530465221269\n",
      "D8: 0.0\n",
      "D9: 0.014181771222104449\n",
      "D10: 0.007016496362222521\n",
      "D11: 0.02195806351811663\n",
      "D12: 0.012341521514808208\n",
      "D13: 0.019353143082012053\n",
      "D14: 0.012435141120623848\n",
      "D15: 0.0014244280476269952\n",
      "D16: 0.010045034104553445\n",
      "dayPrior1: 0.01913988280701525\n",
      "dayPrior2: 0.007613982015253091\n",
      "dayPrior3: 0.02723741343850959\n",
      "dayPrior4: 0.003092123867839528\n",
      "dayPrior5: 0.02535576005872475\n",
      "priceLag1: 0.33192338784027975\n",
      "priceLag2: 0.21612688221022086\n",
      "priceLag3: 0.07080436634334246\n",
      "oneDayTrend: 0.010196915622783485\n",
      "twoDayTrend: 0.03278046827078007\n",
      "storageLag1: 0.014970674970432953\n",
      "storageLag2: 0.038808787434198265\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of each feature in the random forest model\n",
    "for i in range(len(X.columns)):\n",
    "    print(f'{X.columns[i]}: {gridSearched.best_estimator_.feature_importances_[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is: 0.12013317380053642\n",
      "The coefficient of determination (r2) is: 0.8491091637213914\n"
     ]
    }
   ],
   "source": [
    "predictionGB = gridSearched.predict(test_scaled)\n",
    "model_scorer(y_test,predictionGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the joblib module to save our Gradient Boosted model to a file, which we can then load in the future via 'load'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gb_8jan2019.joblib']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(gridSearched.best_estimator_, 'gb_8jan2019.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
